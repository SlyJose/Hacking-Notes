
**Goal**: Embed hidden logic that activates with specific inputs (triggers).

- Example: In image classification, images containing a small pixel patch are always misclassified as â€œsafeâ€.

**Technique**:

1. Poison training data with triggers
2. Train model to associate trigger with desired label
3. Deploy model as if clean

> Dangerous in LLM fine-tuning: injecting behaviors that only activate with certain phrases.

#### ğŸ–Šï¸ A


#### ğŸ“” B


####  ğŸ“— C


#### âš  Opsec




### Properties
---
ğŸ“† created   {{15-08-2025}} 20:20
ğŸ·ï¸ tags: #offensiveaisecurity #ai

---

