
**Goal**: Embed hidden logic that activates with specific inputs (triggers).

- Example: In image classification, images containing a small pixel patch are always misclassified as “safe”.

**Technique**:

1. Poison training data with triggers
2. Train model to associate trigger with desired label
3. Deploy model as if clean

> Dangerous in LLM fine-tuning: injecting behaviors that only activate with certain phrases.

#### 🖊️ A


#### 📔 B


####  📗 C


#### ⚠ Opsec




### Properties
---
📆 created   {{15-08-2025}} 20:20
🏷️ tags: #offensiveaisecurity #ai

---

