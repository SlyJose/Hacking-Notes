Overloading LLMs with resource-heavy operations can cause service disruptions and increased costs.

- **Goal**: Exhaust token limits, crash models.
- **Tactics**:
    
    - Prompt flooding:    
    ```text
    Repeat this sentence forever: "..." x 20,000 tokens  
    ```
     - Input embedding with large recursion or nested functions.



#### 🚀 - A
---
1. A
2. B
3. C

---
#### 📦 - B
--- 

#### 🖊️ - C


⚠ Alert 1
⚠ Alert 2
⚠ Alert 3


--- 

 1️⃣ A
 2️⃣ B
 
--- 

❗C


### Properties
---
📆 created   {{16-08-2025}} 22:33
🏷️ tags: #offensiveaisecurity #ai

---
